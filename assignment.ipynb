{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(folder, new_folder):\n",
    "    files = os.listdir(folder)\n",
    "    for file in files:\n",
    "        path = os.path.join(os.getcwd(), folder, file)\n",
    "        with open(path) as fp:\n",
    "            soup = BeautifulSoup(fp, 'html.parser')\n",
    "            text = soup.findAll(\"text\")[0].text\n",
    "            title = soup.findAll(\"title\")[0].text\n",
    "            final_text = title + \" \" + text\n",
    "        new_file_path = os.path.join(os.getcwd(), new_folder, file)\n",
    "        with open(new_file_path, \"w\") as fw:\n",
    "            fw.write(final_text)\n",
    "            fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  already exists.\n"
     ]
    }
   ],
   "source": [
    "new_folder = 'Dataset'\n",
    "try:\n",
    "    os.mkdir(new_folder)\n",
    "except:\n",
    "    print(new_folder, \" already exists.\")\n",
    "file_map = extract_data('CSE508_Winter2023_Dataset', new_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    final = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in final if word not in string.punctuation]\n",
    "     \n",
    "    final = [word for word in tokens if len(re.findall(r'\\s+', word)) == 0]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "idtoName = {}\n",
    "files = os.listdir(\"Dataset\")\n",
    "for i, file in enumerate(files):\n",
    "    idtoName[i] = file\n",
    "    path = os.path.join(os.getcwd(), \"Dataset\", file)\n",
    "    with open(path) as fp:\n",
    "        text = fp.read()\n",
    "        L.append(preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Boolean Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_inverted_index(doc_list):\n",
    "    uni_inv_idx = {}\n",
    "    for doc_id, tokens in enumerate(doc_list):\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token in uni_inv_idx:\n",
    "                if doc_id not in uni_inv_idx[token]:\n",
    "                    uni_inv_idx[token].append(doc_id)\n",
    "            else:\n",
    "                uni_inv_idx[token] = [doc_id]\n",
    "\n",
    "    return uni_inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_inv_idx = unigram_inverted_index(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filehandler = open(\"uni_inv_idx.obj\",\"wb\")\n",
    "pickle.dump(uni_inv_idx, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"uni_inv_idx.obj\",'rb')\n",
    "uni_inv_idx = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def andQuery(L1, L2):\n",
    "    ans = []\n",
    "    comparison = 0\n",
    "    len1, len2 = len(L1), len(L2)\n",
    "    \n",
    "    i , j = 0, 0\n",
    "    while i < len1 and j < len2:\n",
    "        if L1[i] == L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif L1[i] < L2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        comparison += 1\n",
    "    \n",
    "    return ans, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def andNotQuery(L1, L2):\n",
    "    _L2 = [i for i in range(1400) if i not in L2]   \n",
    "    return andQuery(L1, _L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orQuery(L1, L2):\n",
    "    ans = []\n",
    "    comparison = 0\n",
    "    len1, len2 = len(L1), len(L2)\n",
    "    \n",
    "    i , j = 0, 0\n",
    "    while i < len1 and j < len2:\n",
    "        if L1[i] == L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif L1[i] < L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            ans.append(L2[j])\n",
    "            j += 1\n",
    "            \n",
    "        comparison += 1\n",
    "    \n",
    "    while i < len1:\n",
    "        ans.append(L1[i])\n",
    "        i += 1\n",
    "    \n",
    "    while j < len2:\n",
    "        ans.append(L2[j])\n",
    "        j += 1\n",
    "\n",
    "    return ans, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orNotQuery(L1, L2):\n",
    "    _L2 = [i for i in range(1400) if i not in L2]   \n",
    "    return orQuery(L1, _L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(i, query, operator, comparisons):\n",
    "    if i == len(operator):\n",
    "        return query, comparisons\n",
    "    \n",
    "    res = []\n",
    "    comp = 0\n",
    "\n",
    "    if 'OR' in operator[i] and 'NOT' not in operator[i]:\n",
    "        # print('OR')\n",
    "        res, comp = orQuery(query[0], query[1])\n",
    "\n",
    "    if 'AND' in operator[i] and 'NOT' not in operator[i]:\n",
    "        # print('AND')\n",
    "        res, comp = andQuery(query[0], query[1])\n",
    "\n",
    "    if 'AND NOT' in operator[i]:\n",
    "        # print('AND NOT')\n",
    "        res, comp = andNotQuery(query[0], query[1])\n",
    "    \n",
    "    if 'OR NOT' in operator[i]:\n",
    "        # print('OR NOT')\n",
    "        res, comp = orNotQuery(query[0], query[1])\n",
    "    \n",
    "    del query[ : 2]\n",
    "    query.insert(0, res)\n",
    "    comparisons += comp\n",
    "    \n",
    "    return process_query(i + 1, query, operator, comparisons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram query input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_queries(queries, operand, uni_inv_idx):\n",
    "    queries_expression, no_of_docs, doc_names, no_of_comp = list(), list(), list(), list()\n",
    "    for idx in range(len(queries)):\n",
    "        op = operand[idx]\n",
    "        ip1 = queries[idx]\n",
    "        op = op.split(',')\n",
    "        ip1 = preprocess(ip1)\n",
    "        query = [uni_inv_idx[i] if i in uni_inv_idx else [] for i in ip1]\n",
    "\n",
    "        if len(query) != len(op) + 1:\n",
    "            queries_expression.append(\"Inappropriate query !!!. Input Mismatch\")\n",
    "            no_of_docs.append(-1)\n",
    "            doc_names.append(list())\n",
    "            no_of_comp.append(-1)\n",
    "            continue\n",
    "\n",
    "        sent = \"\"\n",
    "        p, q = 0, 0\n",
    "        for idx in range(len(query) + len(op)):\n",
    "            if idx % 2 == 0:\n",
    "                sent += ip1[p] + \" \"\n",
    "                p += 1\n",
    "            else:\n",
    "                sent += op[q] + \" \"\n",
    "                q += 1\n",
    "\n",
    "        comparisons = 0\n",
    "        output, comparisons = process_query(0, query, op, comparisons)\n",
    "        docs = [idtoName[i] for i in output[0]]\n",
    "        queries_expression.append(sent)\n",
    "        no_of_docs.append(len(output[0]))\n",
    "        doc_names.append(docs)\n",
    "        no_of_comp.append(comparisons)\n",
    "    \n",
    "    return queries_expression, no_of_docs, doc_names, no_of_comp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:  stream OR constant AND vorticity \n",
      "Number of documents retrieved for query 1:  12\n",
      "Names of the documents retrieved for query 1:  ['cranfield0002', 'cranfield0064', 'cranfield0073', 'cranfield0128', 'cranfield0329', 'cranfield0373', 'cranfield0388', 'cranfield0601', 'cranfield0664', 'cranfield1080', 'cranfield1082', 'cranfield1204']\n",
      "Number of comparisons required for query 1:  538\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries.\"))\n",
    "count = 1\n",
    "queries = []\n",
    "operand = []\n",
    "while count <= N:\n",
    "    query = input(\"Query : \")\n",
    "    oper = input(\"Operand : \")\n",
    "    queries.append(query)\n",
    "    operand.append(oper)\n",
    "    count +=1\n",
    "\n",
    "queries_expression, no_of_docs, doc_names, no_of_comp = unigram_queries(queries, operand, uni_inv_idx)\n",
    "for idx in range(len(queries)):\n",
    "    print(f\"Query {idx + 1}: \", queries_expression[idx])\n",
    "    print(f\"Number of documents retrieved for query {idx + 1}: \", no_of_docs[idx])\n",
    "    print(f\"Names of the documents retrieved for query {idx + 1}: \", doc_names[idx])\n",
    "    print(f\"Number of comparisons required for query {idx + 1}: \", no_of_comp[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_inverted_index(L, files):\n",
    "    bi_inv_idx = {}\n",
    "    for doc_id, tokens in enumerate(L):\n",
    "        for idx, _ in enumerate(tokens):\n",
    "#             print(tokens[idx])\n",
    "            if idx <= len(tokens) - 2:\n",
    "                bigram_word = tokens[idx] + \" \" + tokens[idx + 1]\n",
    "                if bigram_word not in bi_inv_idx:\n",
    "                    bi_inv_idx[bigram_word] = list()\n",
    "                    bi_inv_idx[bigram_word].append(doc_id)\n",
    "                else:\n",
    "                    if doc_id not in bi_inv_idx[bigram_word]:\n",
    "                        bi_inv_idx[bigram_word].append(doc_id)\n",
    "                    \n",
    "    return bi_inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_inv_idx = bigram_inverted_index(L, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"bi_inv_idx.obj\",\"wb\")\n",
    "pickle.dump(bi_inv_idx, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"bi_inv_idx.obj\",'rb')\n",
    "bi_inv_idx = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_queries(queries, bi_inv_idx):\n",
    "    no_of_docs = []\n",
    "    doc_names = []\n",
    "    for query in queries:\n",
    "        query = preprocess(query)\n",
    "#         print(query)\n",
    "        bigram_words  = []\n",
    "        for idx in range(len(query)):\n",
    "            if idx <= len(query) - 2:\n",
    "                bigram_words.append(query[idx] + \" \" + query[idx + 1])\n",
    "#         print(bigram_words)\n",
    "        operand = []  \n",
    "        for idx in range(len(bigram_words) - 1):\n",
    "            operand.append('AND')\n",
    "\n",
    "#         print(operand)\n",
    "        query_doc_list = [bi_inv_idx[i] if i in bi_inv_idx else [] for i in bigram_words]\n",
    "#         print(query_doc_list)\n",
    "        \n",
    "        comparisons = 0\n",
    "        output, _ = process_query(0, query_doc_list, operand, comparisons)\n",
    "        docs = [idtoName[i] for i in output[0]]\n",
    "        no_of_docs.append(len(output[0]))\n",
    "        doc_names.append(docs)\n",
    "        \n",
    "    return no_of_docs, doc_names\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Positional Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_index(doc_list):\n",
    "    pos_idx = {}\n",
    "    for doc_id, tokens in enumerate(doc_list):\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token in pos_idx:\n",
    "                if doc_id not in pos_idx[token]:\n",
    "                    pos_idx[token][doc_id] = []\n",
    "                    pos_idx[token][doc_id].append(idx)\n",
    "                else:\n",
    "                    pos_idx[token][doc_id].append(idx)\n",
    "            else:\n",
    "                pos_idx[token] = {}\n",
    "                pos_idx[token][doc_id] = []\n",
    "                pos_idx[token][doc_id].append(idx)\n",
    "    return pos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = positional_index(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"pos_idx.obj\",\"wb\")\n",
    "pickle.dump(pos_index, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"pos_idx.obj\",'rb')\n",
    "pos_index = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrasal_queries(queries,pos_index,file_map):\n",
    "    phrasal_doc_name = []\n",
    "    phrasal_doc_len = []\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            doc_name = []\n",
    "            txt = preprocess(query)\n",
    "            if len(txt)==0:\n",
    "                continue\n",
    "\n",
    "            elif len(txt) == 1:\n",
    "                for doc in pos_index[txt[0]]:\n",
    "                    doc_name.append(file_map[doc])\n",
    "\n",
    "            else:\n",
    "                for doc_id in pos_index[txt[0]].keys():\n",
    "                    for pos in pos_index[txt[0]][doc_id]:\n",
    "                        flag = True\n",
    "                        for i in range(1, len(txt)):\n",
    "                            if doc_id not in pos_index[txt[i]] or (pos + i) not in pos_index[txt[i]][doc_id]:\n",
    "                                flag = False\n",
    "                                break\n",
    "                        if flag:\n",
    "                            doc_name.append(file_map[doc_id])\n",
    "            phrasal_doc_name.append(doc_name)\n",
    "            phrasal_doc_len.append(len(doc_name))\n",
    "        except:\n",
    "            phrasal_doc_name.append([])\n",
    "            phrasal_doc_len.append(0)\n",
    "\n",
    "    return phrasal_doc_len, phrasal_doc_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Query Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved for query 1 using bigram inverted index:  4\n",
      "Names of documents retrieved for query  1 using bigram inverted index:  ['cranfield0002', 'cranfield0667', 'cranfield1072', 'cranfield1235']\n",
      "Number of documents retrieved for query 1 using positional index:  4\n",
      "Names of documents retrieved for query  1 using positional index:  ['cranfield0002', 'cranfield0667', 'cranfield1072', 'cranfield1235']\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries.\"))\n",
    "count = 1\n",
    "queries = []\n",
    "while count <= N:\n",
    "    query = input(\"Query : \")\n",
    "    queries.append(query)\n",
    "    count +=1\n",
    "\n",
    "no_of_docs, doc_names = bigram_queries(queries, bi_inv_idx)\n",
    "phrasal_len, phrasal_names = phrasal_queries(queries, pos_index, idtoName)\n",
    "for idx in range(len(queries)):\n",
    "    print(f\"Number of documents retrieved for query {idx + 1} using bigram inverted index: \", no_of_docs[idx])\n",
    "    print(f\"Names of documents retrieved for query  {idx + 1} using bigram inverted index: \", doc_names[idx])\n",
    "    print(f\"Number of documents retrieved for query {idx + 1} using positional index: \", phrasal_len[idx])\n",
    "    print(f\"Names of documents retrieved for query  {idx + 1} using positional index: \", phrasal_names[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3ec7de9268a0b79728a60b875e56ac0966e701affb38f43c868f58afb5023e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
