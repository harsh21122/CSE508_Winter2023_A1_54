{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/harsh/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(folder, new_folder):\n",
    "    files = os.listdir(folder)\n",
    "    for file in files:\n",
    "        path = os.path.join(os.getcwd(), folder, file)\n",
    "        with open(path) as fp:\n",
    "            soup = BeautifulSoup(fp, 'html.parser')\n",
    "            text = soup.findAll(\"text\")[0].text\n",
    "            title = soup.findAll(\"title\")[0].text\n",
    "            final_text = title + \" \" + text\n",
    "        new_file_path = os.path.join(os.getcwd(), new_folder, file)\n",
    "        with open(new_file_path, \"w\") as fw:\n",
    "            fw.write(final_text)\n",
    "            fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  already exists.\n"
     ]
    }
   ],
   "source": [
    "new_folder = 'Dataset'\n",
    "try:\n",
    "    os.mkdir(new_folder)\n",
    "except:\n",
    "    print(new_folder, \" already exists.\")\n",
    "extract_data('CSE508_Winter2023_Dataset', new_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    final = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in final if word not in string.punctuation]\n",
    "     \n",
    "    final = [word for word in tokens if len(re.findall(r'\\s+', word)) == 0]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "idtoName = {}\n",
    "files = os.listdir(\"Dataset\")\n",
    "for i, file in enumerate(files):\n",
    "    idtoName[i] = file\n",
    "    path = os.path.join(os.getcwd(), \"Dataset\", file)\n",
    "    with open(path) as fp:\n",
    "        text = fp.read()\n",
    "        L.append(preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Boolean Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_inverted_index(doc_list):\n",
    "    uni_inv_idx = {}\n",
    "    for doc_id, tokens in enumerate(doc_list):\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token in uni_inv_idx:\n",
    "                if doc_id not in uni_inv_idx[token]:\n",
    "                    uni_inv_idx[token].append(doc_id)\n",
    "            else:\n",
    "                uni_inv_idx[token] = [doc_id]\n",
    "\n",
    "    return uni_inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_inv_idx = unigram_inverted_index(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filehandler = open(\"uni_inv_idx.obj\",\"wb\")\n",
    "pickle.dump(uni_inv_idx, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"uni_inv_idx.obj\",'rb')\n",
    "uni_inv_idx = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def andQuery(L1, L2):\n",
    "    ans = []\n",
    "    comparison = 0\n",
    "    len1, len2 = len(L1), len(L2)\n",
    "    \n",
    "    i , j = 0, 0\n",
    "    while i < len1 and j < len2:\n",
    "        if L1[i] == L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif L1[i] < L2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        comparison += 1\n",
    "    \n",
    "    return ans, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def andNotQuery(L1, L2):\n",
    "    _L2 = [i for i in range(1400) if i not in L2]   \n",
    "    return andQuery(L1, _L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orQuery(L1, L2):\n",
    "    ans = []\n",
    "    comparison = 0\n",
    "    len1, len2 = len(L1), len(L2)\n",
    "    \n",
    "    i , j = 0, 0\n",
    "    while i < len1 and j < len2:\n",
    "        if L1[i] == L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif L1[i] < L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            ans.append(L2[j])\n",
    "            j += 1\n",
    "            \n",
    "        comparison += 1\n",
    "    \n",
    "    while i < len1:\n",
    "        ans.append(L1[i])\n",
    "        i += 1\n",
    "    \n",
    "    while j < len2:\n",
    "        ans.append(L2[j])\n",
    "        j += 1\n",
    "\n",
    "    return ans, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orNotQuery(L1, L2):\n",
    "    _L2 = [i for i in range(1400) if i not in L2]   \n",
    "    return orQuery(L1, _L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(i, query, operator, comparisons):\n",
    "    if i == len(operator):\n",
    "        return query, comparisons\n",
    "    \n",
    "    res = []\n",
    "    comp = 0\n",
    "\n",
    "    if 'OR' in operator[i] and 'NOT' not in operator[i]:\n",
    "        # print('OR')\n",
    "        res, comp = orQuery(query[0], query[1])\n",
    "\n",
    "    if 'AND' in operator[i] and 'NOT' not in operator[i]:\n",
    "        # print('AND')\n",
    "        res, comp = andQuery(query[0], query[1])\n",
    "\n",
    "    if 'AND NOT' in operator[i]:\n",
    "        # print('AND NOT')\n",
    "        res, comp = andNotQuery(query[0], query[1])\n",
    "    \n",
    "    if 'OR NOT' in operator[i]:\n",
    "        # print('OR NOT')\n",
    "        res, comp = orNotQuery(query[0], query[1])\n",
    "    \n",
    "    del query[ : 2]\n",
    "    query.insert(0, res)\n",
    "    comparisons += comp\n",
    "    \n",
    "    return process_query(i + 1, query, operator, comparisons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram query input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_queries(queries, operand, uni_inv_idx):\n",
    "    queries_expression, no_of_docs, doc_names, no_of_comp = list(), list(), list(), list()\n",
    "    for idx in range(len(queries)):\n",
    "        op = operand[idx]\n",
    "        ip1 = queries[idx]\n",
    "        op = op.split(',')\n",
    "        ip1 = preprocess(ip1)\n",
    "        query = [uni_inv_idx[i] if i in uni_inv_idx else [] for i in ip1]\n",
    "\n",
    "        if len(query) != len(op) + 1:\n",
    "            queries_expression.append(\"Inappropriate query !!!. Input Mismatch\")\n",
    "            no_of_docs.append(-1)\n",
    "            doc_names.append(list())\n",
    "            no_of_comp.append(-1)\n",
    "            continue\n",
    "\n",
    "        sent = \"\"\n",
    "        p, q = 0, 0\n",
    "        for idx in range(len(query) + len(op)):\n",
    "            if idx % 2 == 0:\n",
    "                sent += ip1[p] + \" \"\n",
    "                p += 1\n",
    "            else:\n",
    "                sent += op[q] + \" \"\n",
    "                q += 1\n",
    "\n",
    "        comparisons = 0\n",
    "        output, comparisons = process_query(0, query, op, comparisons)\n",
    "        docs = [idtoName[i] for i in output[0]]\n",
    "        queries_expression.append(sent)\n",
    "        no_of_docs.append(len(output[0]))\n",
    "        doc_names.append(docs)\n",
    "        no_of_comp.append(comparisons)\n",
    "    \n",
    "    return queries_expression, no_of_docs, doc_names, no_of_comp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries.1\n",
      "Query : one two three\n",
      "Operand : OR,AND\n",
      "Query 1:  one OR two AND three \n",
      "Number of documents retrieved for query 1:  36\n",
      "Names of the documents retrieved for query 1:  ['cranfield0750', 'cranfield0536', 'cranfield0641', 'cranfield0679', 'cranfield1357', 'cranfield1392', 'cranfield1351', 'cranfield0609', 'cranfield0059', 'cranfield0456', 'cranfield1125', 'cranfield0140', 'cranfield1092', 'cranfield0773', 'cranfield0582', 'cranfield0364', 'cranfield0791', 'cranfield1049', 'cranfield0844', 'cranfield0620', 'cranfield0673', 'cranfield0476', 'cranfield0672', 'cranfield0006', 'cranfield0454', 'cranfield1145', 'cranfield0064', 'cranfield0856', 'cranfield0660', 'cranfield1325', 'cranfield0266', 'cranfield1144', 'cranfield1036', 'cranfield0785', 'cranfield0948', 'cranfield0187']\n",
      "Number of comparisons required for query 1:  890\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries.\"))\n",
    "count = 1\n",
    "queries = []\n",
    "operand = []\n",
    "while count <= N:\n",
    "    query = input(\"Query : \")\n",
    "    oper = input(\"Operand : \")\n",
    "    queries.append(query)\n",
    "    operand.append(oper)\n",
    "    count +=1\n",
    "\n",
    "queries_expression, no_of_docs, doc_names, no_of_comp = unigram_queries(queries, operand, uni_inv_idx)\n",
    "for idx in range(len(queries)):\n",
    "    print(f\"Query {idx + 1}: \", queries_expression[idx])\n",
    "    print(f\"Number of documents retrieved for query {idx + 1}: \", no_of_docs[idx])\n",
    "    print(f\"Names of the documents retrieved for query {idx + 1}: \", doc_names[idx])\n",
    "    print(f\"Number of comparisons required for query {idx + 1}: \", no_of_comp[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_inverted_index(L, files):\n",
    "    bi_inv_idx = {}\n",
    "    for doc_id, tokens in enumerate(L):\n",
    "        for idx, _ in enumerate(tokens):\n",
    "#             print(tokens[idx])\n",
    "            if idx <= len(tokens) - 2:\n",
    "                bigram_word = tokens[idx] + \" \" + tokens[idx + 1]\n",
    "                if bigram_word not in bi_inv_idx:\n",
    "                    bi_inv_idx[bigram_word] = list()\n",
    "                    bi_inv_idx[bigram_word].append(doc_id)\n",
    "                else:\n",
    "                    if doc_id not in bi_inv_idx[bigram_word]:\n",
    "                        bi_inv_idx[bigram_word].append(doc_id)\n",
    "                    \n",
    "    return bi_inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_inv_idx = bigram_inverted_index(L, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"bi_inv_idx.obj\",\"wb\")\n",
    "pickle.dump(bi_inv_idx, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"bi_inv_idx.obj\",'rb')\n",
    "bi_inv_idx = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_queries(queries, bi_inv_idx):\n",
    "    no_of_docs = []\n",
    "    doc_names = []\n",
    "    for query in queries:\n",
    "        query = preprocess(query)\n",
    "#         print(query)\n",
    "        bigram_words  = []\n",
    "        for idx in range(len(query)):\n",
    "            if idx <= len(query) - 2:\n",
    "                bigram_words.append(query[idx] + \" \" + query[idx + 1])\n",
    "#         print(bigram_words)\n",
    "        operand = []  \n",
    "        for idx in range(len(bigram_words) - 1):\n",
    "            operand.append('AND')\n",
    "\n",
    "#         print(operand)\n",
    "        query_doc_list = [bi_inv_idx[i] if i in bi_inv_idx else [] for i in bigram_words]\n",
    "#         print(query_doc_list)\n",
    "        \n",
    "        comparisons = 0\n",
    "        output, _ = process_query(0, query_doc_list, operand, comparisons)\n",
    "        docs = [idtoName[i] for i in output[0]]\n",
    "        no_of_docs.append(len(output[0]))\n",
    "        doc_names.append(docs)\n",
    "        \n",
    "    return no_of_docs, doc_names\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Query Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries.1\n",
      "Query : free-flight measurements static\n",
      "Number of documents retrieved for query 1 using bigram inverted index:  10\n",
      "Names of documents retrieved for query  1 using bigram inverted index:  ['cranfield1011', 'cranfield1010', 'cranfield1003', 'cranfield1004', 'cranfield1005', 'cranfield1009', 'cranfield1007', 'cranfield1000', 'cranfield1006', 'cranfield1008']\n"
     ]
    }
   ],
   "source": [
    "N = int(input(\"Enter the number of queries.\"))\n",
    "count = 1\n",
    "queries = []\n",
    "while count <= N:\n",
    "    query = input(\"Query : \")\n",
    "    queries.append(query)\n",
    "    count +=1\n",
    "\n",
    "no_of_docs, doc_names = bigram_queries(queries, bi_inv_idx)\n",
    "for idx in range(len(queries)):\n",
    "    print(f\"Number of documents retrieved for query {idx + 1} using bigram inverted index: \", no_of_docs[idx])\n",
    "    print(f\"Names of documents retrieved for query  {idx + 1} using bigram inverted index: \", doc_names[idx])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0131fdb4db9bbc9e9bfddf700cd14f3762505ee7c9a4169e8dd9cce1a46602ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
