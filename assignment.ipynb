{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(folder, new_folder):\n",
    "    files = os.listdir(folder)\n",
    "    for file in files:\n",
    "        path = os.path.join(os.getcwd(), folder, file)\n",
    "        with open(path) as fp:\n",
    "            soup = BeautifulSoup(fp, 'html.parser')\n",
    "            text = soup.findAll(\"text\")[0].text\n",
    "            title = soup.findAll(\"title\")[0].text\n",
    "            final_text = title + \" \" + text\n",
    "        new_file_path = os.path.join(os.getcwd(), new_folder, file)\n",
    "        with open(new_file_path, \"w\") as fw:\n",
    "            fw.write(final_text)\n",
    "            fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  already exists.\n"
     ]
    }
   ],
   "source": [
    "new_folder = 'Dataset'\n",
    "try:\n",
    "    os.mkdir(new_folder)\n",
    "except:\n",
    "    print(new_folder, \" already exists.\")\n",
    "extract_data('CSE508_Winter2023_Dataset', new_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    final = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in final if word not in string.punctuation]\n",
    "     \n",
    "    final = [word for word in tokens if len(re.findall(r'\\s+', word)) == 0]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "idtoName = {}\n",
    "files = os.listdir(\"Dataset\")\n",
    "for i, file in enumerate(files):\n",
    "    idtoName[i] = file\n",
    "    path = os.path.join(os.getcwd(), \"Dataset\", file)\n",
    "    with open(path) as fp:\n",
    "        text = fp.read()\n",
    "        L.append(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_inverted_index(doc_list):\n",
    "    uni_inv_idx = {}\n",
    "    for doc_id, tokens in enumerate(doc_list):\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token in uni_inv_idx:\n",
    "                if doc_id not in uni_inv_idx[token]:\n",
    "                    uni_inv_idx[token].append(doc_id)\n",
    "            else:\n",
    "                uni_inv_idx[token] = [doc_id]\n",
    "\n",
    "    return uni_inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_inv_idx = unigram_inverted_index(L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filehandler = open(\"uni_inv_idx.obj\",\"wb\")\n",
    "pickle.dump(uni_inv_idx, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"uni_inv_idx.obj\",'rb')\n",
    "uni_inv_idx = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def andQuery(L1, L2):\n",
    "    ans = []\n",
    "    comparison = 0\n",
    "    len1, len2 = len(L1), len(L2)\n",
    "    \n",
    "    i , j = 0, 0\n",
    "    while i < len1 and j < len2:\n",
    "        if L1[i] == L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif L1[i] < L2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        comparison += 1\n",
    "    \n",
    "    return ans, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def andNotQuery(L1, L2):\n",
    "    _L2 = [i for i in range(1400) if i not in L2]   \n",
    "    return andQuery(L1, _L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orQuery(L1, L2):\n",
    "    ans = []\n",
    "    comparison = 0\n",
    "    len1, len2 = len(L1), len(L2)\n",
    "    \n",
    "    i , j = 0, 0\n",
    "    while i < len1 and j < len2:\n",
    "        if L1[i] == L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif L1[i] < L2[j]:\n",
    "            ans.append(L1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            ans.append(L2[j])\n",
    "            j += 1\n",
    "            \n",
    "        comparison += 1\n",
    "    \n",
    "    while i < len1:\n",
    "        ans.append(L1[i])\n",
    "        i += 1\n",
    "    \n",
    "    while j < len2:\n",
    "        ans.append(L2[j])\n",
    "        j += 1\n",
    "\n",
    "    return ans, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orNotQuery(L1, L2):\n",
    "    _L2 = [i for i in range(1400) if i not in L2]   \n",
    "    return orQuery(L1, _L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(i, query, operator, comparisons):\n",
    "    if i == len(operator):\n",
    "        return query, comparisons\n",
    "    \n",
    "    res = []\n",
    "    comp = 0\n",
    "\n",
    "    if 'OR' in operator[i] and 'NOT' not in operator[i]:\n",
    "        # print('OR')\n",
    "        res, comp = orQuery(query[0], query[1])\n",
    "\n",
    "    if 'AND' in operator[i] and 'NOT' not in operator[i]:\n",
    "        # print('AND')\n",
    "        res, comp = andQuery(query[0], query[1])\n",
    "\n",
    "    if 'AND NOT' in operator[i]:\n",
    "        # print('AND NOT')\n",
    "        res, comp = andNotQuery(query[0], query[1])\n",
    "    \n",
    "    if 'OR NOT' in operator[i]:\n",
    "        # print('OR NOT')\n",
    "        res, comp = orNotQuery(query[0], query[1])\n",
    "    \n",
    "    del query[ : 2]\n",
    "    query.insert(0, res)\n",
    "    comparisons += comp\n",
    "    \n",
    "    return process_query(i + 1, query, operator, comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:  one OR two AND three \n",
      "Number of documents retrieved for query 1:  36\n",
      "Names of the documents retrieved for query 1:  ['cranfield0006', 'cranfield0059', 'cranfield0064', 'cranfield0140', 'cranfield0187', 'cranfield0266', 'cranfield0364', 'cranfield0454', 'cranfield0456', 'cranfield0476', 'cranfield0536', 'cranfield0582', 'cranfield0609', 'cranfield0620', 'cranfield0641', 'cranfield0660', 'cranfield0672', 'cranfield0673', 'cranfield0679', 'cranfield0750', 'cranfield0773', 'cranfield0785', 'cranfield0791', 'cranfield0844', 'cranfield0856', 'cranfield0948', 'cranfield1036', 'cranfield1049', 'cranfield1092', 'cranfield1125', 'cranfield1144', 'cranfield1145', 'cranfield1325', 'cranfield1351', 'cranfield1357', 'cranfield1392']\n",
      "Number of comparisons required for query 1:  888\n"
     ]
    }
   ],
   "source": [
    "N = int(input())\n",
    "count = 1\n",
    "while count <= N:\n",
    "    ip1 = input()\n",
    "    op = input()\n",
    "    op = op.split(',')\n",
    "    ip1 = preprocess(ip1)\n",
    "    query = [uni_inv_idx[i] if i in uni_inv_idx else [] for i in ip1]\n",
    "\n",
    "    if len(query) != len(op) + 1:\n",
    "        print(\"Inappropriate query !!!\")\n",
    "        break\n",
    "    \n",
    "    sent = \"\"\n",
    "    p, q = 0, 0\n",
    "    for idx in range(len(query) + len(op)):\n",
    "        if idx % 2 == 0:\n",
    "            sent += ip1[p] + \" \"\n",
    "            p += 1\n",
    "        else:\n",
    "            sent += op[q] + \" \"\n",
    "            q += 1\n",
    "    \n",
    "    print(f\"Query {count}: \", sent)\n",
    "    comparisons = 0\n",
    "    query, comparisons = process_query(0, query, op, comparisons)\n",
    "    docs = [idtoName[i] for i in query[0]]\n",
    "    print(f\"Number of documents retrieved for query {count}: \", len(query[0]))\n",
    "    print(f\"Names of the documents retrieved for query {count}: \", docs)\n",
    "    print(f\"Number of comparisons required for query {count}: \", comparisons)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0131fdb4db9bbc9e9bfddf700cd14f3762505ee7c9a4169e8dd9cce1a46602ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
